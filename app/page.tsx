'use client';

import { useState, useCallback, useEffect } from 'react';
import { useAudioRecorder } from './hooks/useAudioRecorder';
import { TranscriptionDisplay } from './components/TranscriptionDisplay';
import { MedicalAnalysis } from './components/MedicalAnalysis';
import { RecordingControls } from './components/RecordingControls';
import { VoiceTraining } from './components/VoiceTraining';
import { VoiceRecognitionSetup } from './components/VoiceRecognitionSetup';
import { ManualDialogueInput } from './components/ManualDialogueInput';
import { ConversationJsonUpload } from './components/ConversationJsonUpload';
import { 
  TranscriptionSegment, 
  MedicalAnalysis as MedicalAnalysisType,
  AnalysisType 
} from './types';
import { Download, Trash2, AlertCircle } from 'lucide-react';

export default function PrescriptionAssistant() {
  const {
    isRecording,
    isPaused,
    duration,
    startRecording,
    stopRecording,
    pauseRecording,
    resumeRecording,
    error: recordingError,
  } = useAudioRecorder();

  const [segments, setSegments] = useState<TranscriptionSegment[]>([]);
  const [analyses, setAnalyses] = useState<Map<AnalysisType, MedicalAnalysisType>>(new Map());
  const [isProcessing, setIsProcessing] = useState(false);
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [showVoiceTraining, setShowVoiceTraining] = useState(false);
  const [showVoiceRecognition, setShowVoiceRecognition] = useState(false);
  const [doctorVoiceProfile, setDoctorVoiceProfile] = useState<any>(null);

  // Load saved voice profile
  useEffect(() => {
    const savedProfile = localStorage.getItem('doctorVoiceProfile');
    if (savedProfile) {
      setDoctorVoiceProfile(JSON.parse(savedProfile));
    }
  }, []);

  const handleStartRecording = useCallback(async () => {
    if (!doctorVoiceProfile) {
      setShowVoiceTraining(true);
      return;
    }
    setError(null);
    await startRecording();
  }, [startRecording, doctorVoiceProfile]);

  const handleVoiceTrainingComplete = useCallback((profile: any) => {
    setDoctorVoiceProfile(profile);
    setShowVoiceTraining(false);
  }, []);

  const handleVoiceTrainingCancel = useCallback(() => {
    setShowVoiceTraining(false);
  }, []);

  const handleStopRecording = useCallback(async () => {
    setIsProcessing(true);
    setError(null);

    try {
      const audioBlob = await stopRecording();
      if (!audioBlob) {
        setError('No audio recorded');
        return;
      }

      const reader = new FileReader();
      reader.readAsDataURL(audioBlob);

      reader.onloadend = async () => {
        const base64Audio = reader.result?.toString().split(',')[1];
        if (!base64Audio) {
          setError('Failed to process audio');
          return;
        }

        try {
          // First try Bangla via Soniox
          const responseBangla = await fetch('/api/transcribe-soniox', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              audioContentBase64: base64Audio,
              language: 'bn-BD',
              enableSpeakerDiarization: true,  // optional but recommended
             diarizationSpeakerCount: 2 
            }),
          });

          let dataBangla = await responseBangla.json();
          if (responseBangla.ok && dataBangla.speakerSegments?.length > 0) {
            const newSegments: TranscriptionSegment[] = dataBangla.speakerSegments.map(
              (seg: any) => ({
                speaker: seg.speaker as 'Doctor' | 'Patient',
                text: seg.text,
                timestamp: new Date(),
              })
            );
            setSegments((prev) => [...prev, ...newSegments]);
          } else {
            // Fallback to English via AssemblyAI
            const responseEnglish = await fetch('/api/transcribe-assemblyai', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                audioContent: base64Audio,
                config: { encoding: 'WEBM_OPUS', sampleRateHertz: 48000, languageCode: 'en-US' },
                isTraining: false,
                voiceProfile: doctorVoiceProfile,
              }),
            });

            if (!responseEnglish.ok) {
              const errData = await responseEnglish.json();
              throw new Error(errData.error || 'English transcription failed');
            }

            const dataEnglish = await responseEnglish.json();
            if (dataEnglish.speakerSegments?.length > 0) {
              const newSegments: TranscriptionSegment[] = dataEnglish.speakerSegments.map(
                (seg: any) => ({
                  speaker: seg.speaker as 'Doctor' | 'Patient',
                  text: seg.text,
                  timestamp: new Date(),
                })
              );
              setSegments((prev) => [...prev, ...newSegments]);
            } else {
              setError('No speech detected in audio');
            }
          }
        } catch (err: any) {
          console.error('Transcription error:', err);
          setError(err.message || 'Failed to transcribe audio');
        }
      };

      reader.onerror = () => {
        setError('Failed to read audio file');
      };

    } catch (err: any) {
      console.error('Recording stop error:', err);
      setError(err.message || 'Failed to process recording');
    } finally {
      setIsProcessing(false);
    }
  }, [stopRecording, doctorVoiceProfile]);

 const handleRequestAnalysis = useCallback(async (type: AnalysisType) => {
  if (segments.length === 0) {
    setError('No conversation to analyze. Please record first.');
    return;
  }

  setIsAnalyzing(true);
  setError(null);

  try {
    const conversation = segments.map((seg) => `${seg.speaker}: ${seg.text}`).join('\n\n');

    // 1Ô∏è‚É£ Translate Bangla to English first
    const translationRes = await fetch('/api/translate', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text: conversation, targetLanguage: 'English' }),
    });

    const translationData = await translationRes.json();

    if (!translationRes.ok) {
      throw new Error(translationData.error || 'Translation failed');
    }

    const englishConversation = translationData.text;

    // 2Ô∏è‚É£ Send translated text to /api/analyze
    const response = await fetch('/api/analyze', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ conversation: englishConversation, analysisType: type }),
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(errorData.error || 'Analysis failed');
    }

    const data = await response.json();

    if (data.success && data.analysis) {
      const newAnalysis: MedicalAnalysisType = {
        type,
        content: data.analysis,
        timestamp: new Date(),
        structuredData: data.structuredData,
        bdMedicines: data.bdMedicines,
      };
      setAnalyses((prev) => {
        const updated = new Map(prev);
        updated.set(type, newAnalysis);
        return updated;
      });

      if (type === 'diagnosis' && segments.length > 0) {
        setTimeout(() => handleRequestAnalysis('prescription'), 500);
      }
    }
  } catch (err: any) {
    console.error('Analysis error:', err);
    setError(err.message || 'Failed to analyze conversation');
  } finally {
    setIsAnalyzing(false);
  }
}, [segments]);


  const handleClearAll = useCallback(() => {
    if (confirm('Are you sure you want to clear all data? This cannot be undone.')) {
      setSegments([]);
      setAnalyses(new Map());
      setError(null);
    }
  }, []);

  const handleAddManualSegments = useCallback((newSegments: TranscriptionSegment[]) => {
    setSegments((prev) => [...prev, ...newSegments]);
  }, []);

  const handleLoadJsonConversation = useCallback((newSegments: TranscriptionSegment[]) => {
    setSegments(newSegments);
    setAnalyses(new Map());
    setError(null);
  }, []);

  const handleRetrainVoice = useCallback(() => {
    if (confirm('This will clear your current voice profile and retrain. Continue?')) {
      localStorage.removeItem('doctorVoiceProfile');
      setDoctorVoiceProfile(null);
      setShowVoiceTraining(true);
    }
  }, []);

  const handleExport = useCallback(() => {
    const exportData = {
      timestamp: new Date().toISOString(),
      duration,
      conversation: segments,
      analyses: Array.from(analyses.entries()).map(([type, analysis]) => ({
        type,
        content: analysis.content,
        timestamp: analysis.timestamp,
      })),
    };

    const blob = new Blob([JSON.stringify(exportData, null, 2)], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `prescription-${new Date().toISOString()}.json`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
  }, [segments, analyses, duration]);

  return (
    <div className="min-h-screen bg-gradient-to-br from-blue-50 to-indigo-100 p-6">
      <div className="max-w-7xl mx-auto">
        {/* Header */}
        <div className="mb-6">
          <div className="flex items-center justify-between">
            <div>
              <div className="flex items-center gap-3 mb-2">
                <h1 className="text-4xl font-bold text-gray-800">üè• Prescription Assistant</h1>
                <div className="flex items-center gap-1 bg-purple-100 text-purple-700 px-3 py-1 rounded-full text-sm font-medium">
                  üß† AssemblyAI & Soniox
                </div>
              </div>
              <p className="text-gray-600">
                AI-powered medical conversation transcription with AssemblyAI for English (fallback) and Soniox for Bangla
              </p>
            </div>

            <div className="text-right">
              {doctorVoiceProfile ? (
                <>
                  <div className="flex items-center gap-2 text-green-600 mb-1">
                    <div className="w-2 h-2 bg-green-500 rounded-full animate-pulse"></div>
                    <span className="text-sm font-medium">Voice Recognition Active</span>
                  </div>
                  <div className="text-xs text-gray-600 mb-1">{doctorVoiceProfile.doctorName}</div>
                  <button
                    onClick={() => setShowVoiceRecognition(true)}
                    className="text-xs text-purple-600 hover:text-purple-800 underline"
                  >
                    Retrain Voice Recognition
                  </button>
                </>
              ) : (
                <>
                  <div className="flex items-center gap-2 text-gray-500 mb-1">
                    <div className="w-2 h-2 bg-gray-400 rounded-full"></div>
                    <span className="text-sm font-medium">No Voice Profile</span>
                  </div>
                  <button
                    onClick={() => setShowVoiceRecognition(true)}
                    className="text-xs text-purple-600 hover:text-purple-800 font-medium underline"
                  >
                    Setup Voice Recognition ‚Üí
                  </button>
                </>
              )}
            </div>
          </div>
        </div>

        {/* Error Display */}
        {(error || recordingError) && (
          <div className="mb-6 bg-red-50 border-l-4 border-red-500 p-4 rounded-lg flex items-start gap-3">
            <AlertCircle className="w-5 h-5 text-red-500 flex-shrink-0 mt-0.5" />
            <div>
              <h3 className="font-semibold text-red-800">Error</h3>
              <p className="text-red-700 text-sm">{error || recordingError}</p>
            </div>
            <button onClick={() => setError(null)} className="ml-auto text-red-500 hover:text-red-700">‚úï</button>
          </div>
        )}

        {/* Action Buttons */}
        <div className="mb-6 flex gap-3">
          <ConversationJsonUpload onLoadConversation={handleLoadJsonConversation} />

          <button
            onClick={handleExport}
            disabled={segments.length === 0}
            className="flex items-center gap-2 px-4 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
          >
            <Download className="w-4 h-4" />
            Export Data
          </button>

          <button
            onClick={handleClearAll}
            disabled={segments.length === 0 && analyses.size === 0}
            className="flex items-center gap-2 px-4 py-2 bg-red-500 text-white rounded-lg hover:bg-red-600 disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
          >
            <Trash2 className="w-4 h-4" />
            Clear All
          </button>
        </div>

        {/* Recording Controls */}
        <div className="mb-6">
          <RecordingControls
            isRecording={isRecording}
            isPaused={isPaused}
            duration={duration}
            isProcessing={isProcessing}
            onStart={handleStartRecording}
            onStop={handleStopRecording}
            onPause={pauseRecording}
            onResume={resumeRecording}
          />
        </div>

        {/* Main Content Grid */}
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
          <div className="h-[600px]"><TranscriptionDisplay segments={segments} /></div>
          <div className="h-[600px]">
            <MedicalAnalysis analyses={analyses} onRequestAnalysis={handleRequestAnalysis} isAnalyzing={isAnalyzing} />
          </div>
        </div>

        <div className="mt-6 text-center text-sm text-gray-500">
          <p>This system uses AssemblyAI (English fallback) and Soniox (Bangla) for transcription, and OpenAI for medical conversation analysis.</p>
          <p className="mt-1">‚ö†Ô∏è Always verify AI-generated suggestions. This tool is for clinical decision support only.</p>
        </div>
      </div>

      {showVoiceTraining && (
        <VoiceTraining onTrainingComplete={handleVoiceTrainingComplete} onCancel={handleVoiceTrainingCancel} />
      )}

      {showVoiceRecognition && (
        <VoiceRecognitionSetup
          onSetupComplete={(profile) => { setDoctorVoiceProfile(profile); setShowVoiceRecognition(false); }}
          onCancel={() => setShowVoiceRecognition(false)}
        />
      )}
    </div>
  );
}
